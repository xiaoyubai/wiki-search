{
 "metadata": {
  "name": "",
  "signature": "sha256:5d098539d6cf20739efd436c1b79837b68a070f1c53d375352abd671d7ac7dcc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# nmf is mllib.als with setNonnegative\n",
      "# Kenny was here\n",
      "\n",
      "import sys\n",
      "sys.setrecursionlimit(2 ** 30 -1)\n",
      "sys.getrecursionlimit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 144,
       "text": [
        "1073741823"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pyspark as ps\n",
      "import os\n",
      "import re\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.corpus import stopwords, words, wordnet\n",
      "import string\n",
      "from collections import Counter\n",
      "from pyspark.mllib.clustering import KMeans\n",
      "from pyspark.mllib.feature import HashingTF\n",
      "from pyspark.mllib.feature import IDF\n",
      "from pyspark.mllib.regression import LabeledPoint\n",
      "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
      "from pyspark.mllib.linalg import Vectors, SparseVector\n",
      "from operator import add\n",
      "from pyspark.mllib.feature import Word2Vec\n",
      "\n",
      "# Next step\n",
      "from pyspark.mllib.recommendation import ALS, Rating\n",
      "\n",
      "PUNCTUATION = set(string.punctuation)\n",
      "STOPWORDS = set(stopwords.words('english'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_keys(filename=None):\n",
      "    \"\"\"\n",
      "    load key pairs from bash_profile or \\\n",
      "    manually create a json file with:\n",
      "    cat aws.json\n",
      "    {\"ACCESS_KEY\": \"ACCESS_KEY\", \"SECRET_ACCESS_KEY\": \"SECRET_ACCESS_KEY\"}\n",
      "    Use json.load to load keys\n",
      "    \"\"\"\n",
      "    try:\n",
      "        ACCESS_KEY = os.environ['AWS_ACCESS_KEY_ID']\n",
      "        SECRET_ACCESS_KEY = os.environ['AWS_SECRET_ACCESS_KEY']\n",
      "    except:\n",
      "        with open(filename) as f:\n",
      "            data = json.load(f)\n",
      "            ACCESS_KEY = data['ACCESS_KEY']\n",
      "            SECRET_ACCESS_KEY = data['SECRET_ACCESS_KEY']\n",
      "    return ACCESS_KEY, SECRET_ACCESS_KEY\n",
      "\n",
      "def get_content(article):\n",
      "    try:\n",
      "        title = re.search(r'\\'\\'\\'([\\w+\\s]+)\\'\\'\\'', article).group(1)\n",
      "        return [title, article]\n",
      "    except:\n",
      "        return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenizing(text):\n",
      "    regex = re.compile('<.+?>|[^a-zA-Z]')\n",
      "    clean_txt = regex.sub(' ', text)\n",
      "    tokens = clean_txt.split()\n",
      "    lowercased = [t.lower() for t in tokens]\n",
      "\n",
      "    no_punctuation = []\n",
      "    for word in lowercased:\n",
      "        punct_removed = ''.join([letter for letter in word if not letter in PUNCTUATION])\n",
      "        no_punctuation.append(punct_removed)\n",
      "    no_stopwords = [w for w in no_punctuation if not w in STOPWORDS]\n",
      "\n",
      "    STEMMER = PorterStemmer()\n",
      "    stemmed = [STEMMER.stem(w) for w in no_stopwords]\n",
      "    return [w for w in stemmed if w]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# take n samples\n",
      "first_n_lines = 2000\n",
      "redirect_str = '^#REDIRECT'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ACCESS_KEY, SECRET_ACCESS_KEY = load_keys('../aws.json')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# connect to s3\n",
      "\n",
      "# 10% of articles\n",
      "# link = 's3n://%s:%s@wikisample10/sample2' % (ACCESS_KEY, SECRET_ACCESS_KEY)\n",
      "\n",
      "# whole articles\n",
      "link = 's3n://%s:%s@jyt109/wiki_articles' % (ACCESS_KEY, SECRET_ACCESS_KEY)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wiki_rdd = sc.textFile(link)\n",
      "# wiki_rdd_samples = sc.parallelize(wiki_rdd.take(first_n_lines), 24)\n",
      "wiki_no_redirect_rdd = wiki_rdd.filter(lambda line: '#REDIRECT' not in line)\n",
      "wiki_indexed = wiki_no_redirect_rdd.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
      "token_rdd = wiki_no_redirect_rdd.map(tokenizing)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hashingTF = HashingTF()\n",
      "tf = hashingTF.transform(token_rdd)\n",
      "tf.cache()\n",
      "idf = IDF(minDocFreq=2).fit(tf)\n",
      "tfidf = idf.transform(tf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# def convert_rating(sparse_vector_and_index):\n",
      "#     sparse_vector, index = sparse_vector_and_index\n",
      "#     for key, value in zip(sparse_vector.indices, sparse_vector.values):\n",
      "#         if value != 0:\n",
      "#             yield (index, key, value)\n",
      "# tfidf_rating = tfidf.zipWithIndex().flatMap(convert_rating).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
      "# tfidf_rating.take(1000000)\n",
      "# rank = 20\n",
      "# numIterations = 10\n",
      "# model = ALS.trainImplicit(tfidf_rating, rank, numIterations, alpha=0.01)\n",
      "# index_label = model.userFeatures().map(lambda x: (x[0], np.argmax(x[1])))\n",
      "# index_feature = tfidf.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
      "# index_label_feature = index_label.join(index_feature)\n",
      "# label_feature = index_label_feature.map(lambda x: x[1])\n",
      "# train = label_feature.map(lambda x: LabeledPoint(x[0], x[1]))\n",
      "# model = NaiveBayes.train(train, 1.0)\n",
      "# training, test = train.randomSplit([0.6, 0.4], seed=0)\n",
      "# predictionAndLabel = test.map(lambda p: (model.predict(p.features), p.label))\n",
      "# accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / test.count()\n",
      "# print \"accuracy: \", accuracy\n",
      "# category = 'math math math math statistics'\n",
      "# category_token = tokenizing(category)\n",
      "# category_token\n",
      "# tf_test = hashingTF.transform(category_token)\n",
      "# tf_test\n",
      "# tfidf_test = idf.transform(tf_test)\n",
      "# tfidf_test\n",
      "# model.predict(tfidf_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_title_link(article):\n",
      "    try:\n",
      "        links = re.findall(r'\\[\\[(.*?)[\\]\\]|\\|]', article)\n",
      "        title = re.search(r'\\'\\'\\'(.*?)\\'\\'\\'', article).group(1)\n",
      "        return title, \"|\".join(links)\n",
      "    except:\n",
      "        return \"\", [\"\"]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# links = wiki_rdd.map(get_title_link).filter(lambda line: line[0] != \"\").distinct().cache()\n",
      "# # links = links.map(get_not_nones).cache()\n",
      "# print \"counts, \" links.count()\n",
      "# ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
      "# def computeContribs(urls, rank):\n",
      "#     \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
      "#     url_list = urls.split(\"|\")\n",
      "#     num_urls = len(url_list)\n",
      "#     for url in url_list:\n",
      "#         yield (url, rank / num_urls)\n",
      "# contribs = links.join(ranks).flatMap(lambda url_urls_rank: \\\n",
      "#            computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
      "# # ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
      "# ranks = contribs.reduceByKey(add) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_title(article):\n",
      "    try:\n",
      "        title = re.search(r'\\'\\'\\'(.*?)\\'\\'\\'', article).group(1)\n",
      "        return title\n",
      "    except:\n",
      "        return \" \""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "multi_links = wiki_rdd.filter(lambda line: \"may refer to:\" in line)\n",
      "title_rdd = wiki_no_redirect_rdd.map(get_title)\n",
      "title_index = title_rdd.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
      "tfidf_index = tfidf.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
      "title_tfidf = title_index.join(tfidf_index).map(lambda x: x[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_title_tfidf(title_string):\n",
      "    titles = title_string.split(\"|\")\n",
      "    for title in titles:\n",
      "        yield title"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "keyword = \"recall\"\n",
      "category = \"statistics\"\n",
      "category_token = tokenizing(category)\n",
      "tf_category = hashingTF.transform(category_token)\n",
      "related_links = multi_links.map(get_title_link).filter(lambda x: x[0]==keyword).map(lambda x: x[1]).first().split(\"|\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "related_tfidf = title_tfidf.filter(lambda x: x[0] in related_links).collect()\n",
      "# related_tfidf = title_tfidf.filter(lambda x: x[0] in related_links)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calcuate cosine similarity between two sparse vectors\n",
      "def cosine_sim(v1, v2, origin):\n",
      "    try:\n",
      "        return v1.dot(v2) / (v1.squared_distance(origin) * v2.squared_distance(origin))\n",
      "    except:\n",
      "        return v1.dot(v2) / (v1.squared_distance(origin) * v2.squared_distance(origin) + 1)\n",
      "\n",
      "def max_cosine_sim(related_tfidf, tf_category):\n",
      "    num_cols = len(tf_category)\n",
      "    # initilize a 0 sparse vector to calcuate norm\n",
      "    origin = SparseVector(num_cols, {})\n",
      "    title_cos_sim = np.array([[title, cosine_sim(vector, tf_category, origin)] for title, vector in related_tfidf])\n",
      "    print title_cos_sim\n",
      "    return title_cos_sim[np.argmax(title_cos_sim[:,1]),0]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max_cosine_sim(related_tfidf, tf_category)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}